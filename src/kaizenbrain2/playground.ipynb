{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36',}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    data = re.sub(r'\\([^)]*\\)|\\$|,', '',str(data.strip()))\n",
    "    return data\n",
    "\n",
    "\n",
    "def db_connection():\n",
    "    try:\n",
    "        conn = psycopg2.connect(dbname='kaizen_brain_development', user='timescaledb', password='123456', host='10.0.0.26', port='5432')\n",
    "    except Exception as e:\n",
    "        print(\"Error connecting to the database:\")\n",
    "        print(e)\n",
    "        conn = None\n",
    "    return conn\n",
    "  \n",
    "def query_db(sql):\n",
    "    conn = db_connection()\n",
    "    if conn:\n",
    "        try:\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(sql)\n",
    "                result = cursor.fetchall()\n",
    "        except Exception as e:\n",
    "            print(\"Error executing database query:\")\n",
    "            print(e)\n",
    "            result = None\n",
    "        finally:\n",
    "            conn.close()\n",
    "    else:\n",
    "        result = None\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "2024-04-06 19:19:04 : Starting Main Function\n",
      "\tFetching last saved data(html url) from Database\n",
      "\tlast_html = \n",
      "\tall_data_fetched = False, start = 0\n",
      "\tFetching latest filings from\n",
      "\t\tPage 1, https://www.sec.gov/cgi-bin/browse-edgar?action=getcurrent&&type=4&owner=only&count=40&start=0\n",
      "\t\t\trow 0:New Data,                 html = https://www.sec.gov/Archives/edgar/data/1875240/000121390024030963/0001213900-24-030963-index.htm\n",
      "\t\t\trow 1:New Data,                 html = https://www.sec.gov/Archives/edgar/data/1841330/000121390024030963/0001213900-24-030963-index.htm\n",
      "\t\t\trow 2:New Data,                 html = https://www.sec.gov/Archives/edgar/data/1875691/000121390024030958/0001213900-24-030958-index.htm\n",
      "\t\t\trow 3:New Data,                 html = https://www.sec.gov/Archives/edgar/data/1841330/000121390024030958/0001213900-24-030958-index.htm\n",
      "\t\t\trow 4:New Data,                 html = https://www.sec.gov/Archives/edgar/data/1841330/000121390024030961/0001213900-24-030961-index.htm\n",
      "\t\t\trow 5:New Data,                 html = https://www.sec.gov/Archives/edgar/data/1565536/000121390024030961/0001213900-24-030961-index.htm\n",
      "\t\t\trow 6:New Data,                 html = https://www.sec.gov/Archives/edgar/data/1875180/000121390024030962/0001213900-24-030962-index.htm\n",
      "\t\t\trow 7:New Data,                 html = https://www.sec.gov/Archives/edgar/data/1841330/000121390024030962/0001213900-24-030962-index.htm\n",
      "\t\t\trow 8:New Data,                 html = https://www.sec.gov/Archives/edgar/data/1841330/000121390024030960/0001213900-24-030960-index.htm\n",
      "\t\t\trow 9:New Data,                 html = https://www.sec.gov/Archives/edgar/data/1388862/000121390024030960/0001213900-24-030960-index.htm\n",
      "\t\t\trow 10:New Data,                 html = https://www.sec.gov/Archives/edgar/data/1965006/000121390024030959/0001213900-24-030959-index.htm\n",
      "\t\t\trow 11:New Data,                 html = https://www.sec.gov/Archives/edgar/data/1841330/000121390024030959/0001213900-24-030959-index.htm\n",
      "\t\t\trow 12:New Data,                 html = https://www.sec.gov/Archives/edgar/data/1841330/000121390024030957/0001213900-24-030957-index.htm\n",
      "\t\t\trow 13:New Data,                 html = https://www.sec.gov/Archives/edgar/data/1132679/000121390024030957/0001213900-24-030957-index.htm\n",
      "\t\t\trow 14:New Data,                 html = https://www.sec.gov/Archives/edgar/data/1936224/000095017024042404/0000950170-24-042404-index.htm\n",
      "\t\t\trow 15:New Data,                 html = https://www.sec.gov/Archives/edgar/data/1439316/000095017024042404/0000950170-24-042404-index.htm\n",
      "\t\t\trow 16:New Data,                 html = https://www.sec.gov/Archives/edgar/data/1936224/000095017024042403/0000950170-24-042403-index.htm\n",
      "\t\t\trow 17:New Data,                 html = https://www.sec.gov/Archives/edgar/data/1839253/000095017024042403/0000950170-24-042403-index.htm\n",
      "\t\t\trow 18:New Data,                 html = https://www.sec.gov/Archives/edgar/data/1936224/000095017024042402/0000950170-24-042402-index.htm\n",
      "\t\t\trow 19:New Data,                 html = https://www.sec.gov/Archives/edgar/data/1287248/000095017024042402/0000950170-24-042402-index.htm\n",
      "\t\t\trow 20:New Data,                 html = https://www.sec.gov/Archives/edgar/data/1936224/000095017024042398/0000950170-24-042398-index.htm\n",
      "\t\t\trow 21:New Data,                 html = https://www.sec.gov/Archives/edgar/data/1514853/000095017024042398/0000950170-24-042398-index.htm\n",
      "\t\t\trow 22:New Data,                 html = https://www.sec.gov/Archives/edgar/data/1984081/000095017024042397/0000950170-24-042397-index.htm\n",
      "\t\t\trow 23:New Data,                 html = https://www.sec.gov/Archives/edgar/data/1936224/000095017024042397/0000950170-24-042397-index.htm\n",
      "\t\t\trow 24:New Data,                 html = https://www.sec.gov/Archives/edgar/data/1984494/000095017024042396/0000950170-24-042396-index.htm\n",
      "\t\t\trow 25:New Data,                 html = https://www.sec.gov/Archives/edgar/data/1936224/000095017024042396/0000950170-24-042396-index.htm\n",
      "\t\t\trow 26:New Data,                 html = https://www.sec.gov/Archives/edgar/data/2007798/000095017024042395/0000950170-24-042395-index.htm\n",
      "\t\t\trow 27:New Data,                 html = https://www.sec.gov/Archives/edgar/data/1936224/000095017024042395/0000950170-24-042395-index.htm\n",
      "\t\t\trow 28:New Data,                 html = https://www.sec.gov/Archives/edgar/data/1872792/000141588924010473/0001415889-24-010473-index.htm\n",
      "\t\t\trow 29:New Data,                 html = https://www.sec.gov/Archives/edgar/data/1831828/000141588924010473/0001415889-24-010473-index.htm\n",
      "\t\t\trow 30:New Data,                 html = https://www.sec.gov/Archives/edgar/data/1984003/000095017024042391/0000950170-24-042391-index.htm\n",
      "\t\t\trow 31:New Data,                 html = https://www.sec.gov/Archives/edgar/data/1936224/000095017024042391/0000950170-24-042391-index.htm\n",
      "\t\t\trow 32:New Data,                 html = https://www.sec.gov/Archives/edgar/data/1872792/000141588924010472/0001415889-24-010472-index.htm\n",
      "\t\t\trow 33:New Data,                 html = https://www.sec.gov/Archives/edgar/data/1831828/000141588924010472/0001415889-24-010472-index.htm\n",
      "\t\t\trow 34:New Data,                 html = https://www.sec.gov/Archives/edgar/data/1426950/000112760224012619/0001127602-24-012619-index.htm\n",
      "\t\t\trow 35:New Data,                 html = https://www.sec.gov/Archives/edgar/data/55135/000112760224012619/0001127602-24-012619-index.htm\n",
      "\t\t\trow 36:New Data,                 html = https://www.sec.gov/Archives/edgar/data/2004307/000185781624000075/0001857816-24-000075-index.htm\n",
      "\t\t\trow 37:New Data,                 html = https://www.sec.gov/Archives/edgar/data/2004302/000185781624000075/0001857816-24-000075-index.htm\n",
      "\t\t\trow 38:New Data,                 html = https://www.sec.gov/Archives/edgar/data/2004293/000185781624000075/0001857816-24-000075-index.htm\n",
      "\t\t\trow 39:New Data,                 html = https://www.sec.gov/Archives/edgar/data/1962329/000185781624000075/0001857816-24-000075-index.htm\n",
      "\t\tall_data_fetched = True, start = 40\n",
      "\t40 new latest_filing_data found \n",
      "\tInserting latest_filings Data in Table latest_filing, 40 rows\n",
      "Error inserting data into the database:\n",
      "invalid input syntax for type integer: \"https://www.sec.gov/Archives/edgar/data/1875240/000121390024030963/0001213900-24-030963-index.htm\"\n",
      "LINE 1: INSERT INTO latest_filings VALUES ('https://www.sec.gov/Arch...\n",
      "                                           ^\n",
      "\n",
      "\tFetching Form4 Data\n",
      "\t\t1/40\n",
      "\t\tform4_url = https://www.sec.gov/Archives/edgar/data/1841330/000121390024030963/xslF345X05/ownership.xml\n"
     ]
    }
   ],
   "source": [
    "def insert_table(table_name, data):\n",
    "    conn = db_connection()\n",
    "    if conn:\n",
    "        try:\n",
    "            # Dynamic SQL insert statement, list of column values depends on list data length\n",
    "            sql_insert = f\"INSERT INTO {table_name} VALUES ({', '.join(['%s'] * len(data[0]))})\"\n",
    "            with conn.cursor() as cursor:\n",
    "                if isinstance(data, list) and len(data) > 0:\n",
    "                    cursor.executemany(sql_insert, data)\n",
    "                else:\n",
    "                    cursor.execute(sql_insert, data)\n",
    "            conn.commit()\n",
    "        except Exception as e:\n",
    "            print(\"Error inserting data into the database:\")\n",
    "            print(e)\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "\n",
    "def get_latest_filings():\n",
    "    try:\n",
    "        print(f'\\tFetching last saved data(html url) from Database')\n",
    "        last_html = query_db('SELECT html_url FROM latest_filings ORDER BY accepted_datetime DESC LIMIT 1;')\n",
    "        # If last html_url present in the table or empty table, result = Type list\n",
    "        if isinstance(last_html, list):\n",
    "            # only 1 item, max should be returned\n",
    "            if len(last_html) == 1:\n",
    "                last_html = last_html[0][0]\n",
    "            elif len(last_html) == 0:\n",
    "                last_html = ''\n",
    "        print(f'\\tlast_html = {last_html}')\n",
    "    except Exception as e:\n",
    "        print(\"\\tError while fetching the latest filings:\")\n",
    "        print(e)\n",
    "        last_html = None\n",
    "    latest_filing_data = []\n",
    "    if last_html is not None:\n",
    "        all_data_fetched = False\n",
    "        start = 0\n",
    "        print(f'\\tall_data_fetched = {all_data_fetched}, start = {start}')\n",
    "        print(f'\\tFetching latest filings from')\n",
    "        # Condition to fetch number of pages till last data (html) already saved\n",
    "        #while not all_data_fetched and start < 100:\n",
    "        while not all_data_fetched:\n",
    "            sec_edgar_url = f'https://www.sec.gov/cgi-bin/browse-edgar?action=getcurrent&&type=4&owner=only&count=40&start={start}'\n",
    "            print(f'\\t\\tPage {int(start/40+1)}, {sec_edgar_url}')\n",
    "            response = requests.get(sec_edgar_url, headers=headers)\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            try:\n",
    "                filings_table = soup.div.find_all('table',recursive=False)[1]\n",
    "                filings_table = filings_table.find_all('tr')[1:]\n",
    "                # make pair of table rows, each table row pair belongs to one filing\n",
    "                filings_table_row_pairs = list(zip(filings_table[::2], filings_table[1::2]))\n",
    "                # print(f'\\t\\t Found {len(filings_table_row_pairs)} rows in page : {start+1}')\n",
    "                for item in filings_table_row_pairs:\n",
    "                    # item = filings_table_row_pairs[0]\n",
    "                    description         = item[0].text.strip()\n",
    "                    html                = 'https://www.sec.gov' + item[1].a['href']\n",
    "                    Accepted_Date       = item[1].find_all('td')[3].text[:10]\n",
    "                    Accepted_Time       = item[1].find_all('td')[3].text[10:]\n",
    "                    Accepted_DateTime   = datetime.strptime(f'{Accepted_Date} {Accepted_Time}', '%Y-%m-%d %H:%M:%S')\n",
    "                    Filing_Date         = item[1].find_all('td')[4].text\n",
    "                    try:\n",
    "                        File                = item[1].find_all('td')[5].text.split('\\n')[0]\n",
    "                        Film_No             = int(item[1].find_all('td')[5].text.split('\\n')[1])\n",
    "                    except:\n",
    "                        File    = None\n",
    "                        Film_No = None\n",
    "                    if html != last_html:\n",
    "                        print(f'\\t\\t\\trow {filings_table_row_pairs.index(item)}:New Data,                 html = {html}')\n",
    "                        latest_filing_data.append((html,description,Accepted_DateTime,Filing_Date,File,Film_No))\n",
    "                    elif html == last_html:\n",
    "                        print(f'\\t\\t\\trow {filings_table_row_pairs.index(item)}:Reached last saved data,  html = {html}')\n",
    "                        all_data_fetched = True\n",
    "                        # print(f'\\t\\t\\tall_data_fetched = {all_data_fetched}, start = {start}')\n",
    "                        break\n",
    "                all_data_fetched = True\n",
    "            except Exception as e:\n",
    "                print(f'\\t\\tExcept block, Error in Fetching : {sec_edgar_url}')\n",
    "                print(f'\\t\\t{e}')\n",
    "                all_data_fetched = True\n",
    "                print(f'\\t\\tall_data_fetched = {all_data_fetched}, start = {start}')\n",
    "                print(f'\\t\\thtml = {html}, last_html = {last_html}')\n",
    "            start += 40\n",
    "            print(f'\\t\\tall_data_fetched = {all_data_fetched}, start = {start}')\n",
    "    else:\n",
    "        print('\\t ELSE: Error fetching last saved html url from Database')\n",
    "    return latest_filing_data\n",
    "\n",
    "\n",
    "def get_filing_details(index_html_url : str):\n",
    "    insider_trading_data = []\n",
    "    index_response = requests.get(index_html_url, headers=headers)\n",
    "    if index_response.status_code == 200:\n",
    "        try:\n",
    "            index_soup = BeautifulSoup(index_response.content, \"html.parser\")\n",
    "            accepted_datetime = index_soup.find('div', class_=\"formGrouping\").find_all('div', class_='info')[1].text\n",
    "            index_table = index_soup.find('table')\n",
    "            form4_url = 'https://www.sec.gov' + index_table.a['href']\n",
    "            print(f'\\t\\tform4_url = {form4_url}')\n",
    "            form4_response = requests.get(form4_url, headers=headers)\n",
    "            if form4_response.status_code == 200:\n",
    "                try:\n",
    "                    form4_soup = BeautifulSoup(form4_response.content, \"html.parser\")\n",
    "                    all_tables = form4_soup.body.find_all('table', recursive=False)\n",
    "                    company_data = all_tables[1]\n",
    "                    non_derivative_table = all_tables[2]\n",
    "                    derivative_table = all_tables[3]\n",
    "                    company_data    = company_data.tr.find_all('td', recursive=False)\n",
    "                    insider_name    = company_data[0].td.text\n",
    "                    company_name    = company_data[1].a.text\n",
    "                    ticker          = company_data[1].find_all('span')[1].text\n",
    "                    director        = company_data[2].find_all('td')[0].text\n",
    "                    owner_10        = company_data[2].find_all('td')[2].text\n",
    "                    officer         = company_data[2].find_all('td')[4].text\n",
    "                    other           = company_data[2].find_all('td')[6].text\n",
    "                    if officer or other:\n",
    "                        title       = company_data[2].find_all('td')[8].text.strip()\n",
    "                        if title == \"See Remarks\":\n",
    "                            title   = all_tables[4].find('tr', string='Remarks:').next_sibling.next_sibling.text\n",
    "                    elif director and owner_10:\n",
    "                        title       = 'Director, 10% Owner'\n",
    "                    elif director:\n",
    "                        title       = 'Director'\n",
    "                    elif owner_10:\n",
    "                        title       = '10% Owner'\n",
    "                    # Non-Derivative table\n",
    "                    if non_derivative_table.tbody:\n",
    "                        trade_type_mapping = {'A': 'Buy', 'D': 'Sale'}\n",
    "                        security_table = non_derivative_table.tbody.find_all('tr')\n",
    "                        # row = security_table[0]\n",
    "                        for row in security_table :\n",
    "                            fill_date   = clean_data(row.find_all('td')[1].text)\n",
    "                            if fill_date != '':\n",
    "                                qty         = clean_data(row.find_all('td')[5].text.strip())\n",
    "                                if qty == '':\n",
    "                                    qty     = 0\n",
    "                                else:\n",
    "                                    qty     = float(qty)\n",
    "                                trade_type  = trade_type_mapping.get(clean_data(row.find_all('td')[6].text.strip()), None)\n",
    "                                price       = clean_data(row.find_all('td')[7].text.strip())\n",
    "                                if price == \"\":\n",
    "                                    price   = 0\n",
    "                                else:\n",
    "                                    price   = float(price)\n",
    "                                values      = round(qty * price)\n",
    "                                insider_trading_data.append((fill_date,accepted_datetime,ticker,company_name,insider_name,title,trade_type,price,qty,values,form4_url))\n",
    "                    # Derivative Table\n",
    "                    if derivative_table.tbody:\n",
    "                        derivative_table = derivative_table.tbody.find_all('tr')\n",
    "                        # row = derivative_table[0]\n",
    "                        for row in derivative_table:\n",
    "                            fill_date   = clean_data(row.find_all('td')[2].text.strip())\n",
    "                            if fill_date != '':\n",
    "                                trade_type  = 'Option Exercise'\n",
    "                                qty         = clean_data(row.find_all('td')[6].text.strip())\n",
    "                                if qty == '':\n",
    "                                    qty = 0\n",
    "                                else:\n",
    "                                    qty = float(qty)\n",
    "                                price       = clean_data(row.find_all('td')[12].text.strip())\n",
    "                                if price != \"\":\n",
    "                                    price   = float(price)\n",
    "                                else:\n",
    "                                    price   = 0\n",
    "                                values      = round(qty * price)\n",
    "                                insider_trading_data.append((fill_date,accepted_datetime,ticker,company_name,insider_name,title,trade_type,price,qty,values, form4_url))\n",
    "                    print(f'\\t\\t{fill_date},{accepted_datetime},{ticker},{company_name},{insider_name},{title},{trade_type},{price},{qty},{values}')\n",
    "                    return insider_trading_data\n",
    "                except Exception as e:\n",
    "                    print(f\"Error while fetching form4_url = {form4_url}:\")\n",
    "                    print(e)\n",
    "            else:\n",
    "                print(f'Error in request {form4_url}\\n{form4_response.status_code} {form4_response.text}')\n",
    "        except requests.exceptions.RequestException as re:\n",
    "            print(f\"Request Exception while fetching {index_html_url}:\")\n",
    "            print(re)\n",
    "    return insider_trading_data\n",
    "\n",
    "\n",
    "def save_to_excel(data: list, excel_file=r\"insider_trading.xlsx\"):\n",
    "    headers = [\"Fill Date\", \"Time\", \"Ticker\", \"Company\", \"Insider Name\", \"Title\", \"Trade Type\", \"Price\", \"QTY\", \"Value\", \"SEC URL\"]\n",
    "    data_df = pd.DataFrame(data, columns=headers)\n",
    "    try:\n",
    "        # Check if the file exists or not\n",
    "        file_exists = os.path.isfile(excel_file)\n",
    "        # Try to append data to an existing Excel file or create a new one\n",
    "        if file_exists:\n",
    "            # get last row postion in existing excel file\n",
    "            last_row = pd.read_excel(excel_file).index.stop + 1\n",
    "            with pd.ExcelWriter(excel_file, mode=\"a\", if_sheet_exists=\"overlay\") as writer:\n",
    "                data_df.to_excel(writer, startrow = last_row, index=False, header=False)\n",
    "                print(f'\\t\\tData appended to file {excel_file}.')\n",
    "        else:\n",
    "            with pd.ExcelWriter(excel_file, mode=\"w\") as writer:\n",
    "                data_df.to_excel(writer, index=False)\n",
    "                print(f'\\t\\tNew file {excel_file} created and data saved.')\n",
    "    except PermissionError:\n",
    "        print(f'\\t\\tError: Permission denied while trying to access {excel_file}.')\n",
    "    except Exception as e:\n",
    "        print(f'\\t\\t\\tAn error occurred: {e}')\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(f'\\n\\n{datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")} : Starting Main Function')\n",
    "    latest_filing_data = get_latest_filings()\n",
    "    if latest_filing_data:\n",
    "        print(f'\\t{len(latest_filing_data)} new latest_filing_data found ')\n",
    "        print(f'\\tInserting latest_filings Data in Table latest_filing, {len(latest_filing_data)} rows')\n",
    "        insert_table('latest_filings', latest_filing_data)\n",
    "        html_urls = [item[0] for item in latest_filing_data]\n",
    "        insider_trading_data = []\n",
    "        print(\"\\tFetching Form4 Data\")\n",
    "        for position,html_url in enumerate(html_urls,start=1):\n",
    "            print(f'\\t\\t{position}/{len(html_urls)}')\n",
    "            insider_trading_data = get_filing_details(html_url)\n",
    "            if insider_trading_data:\n",
    "                print(\"\\t\\tInserting Form4 data into insider_trading table\")\n",
    "                insert_table('insider_trading', insider_trading_data)\n",
    "                # save_to_excel(insider_trading_data)\n",
    "        print('Done')\n",
    "    else:\n",
    "        print('\\tNo new data from latest filings since last saved')\n",
    "    print(\"Sleeping for 300 seconds (5 mins)\\n\")\n",
    "    time.sleep(300)\n",
    "\n",
    " \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # while True:\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
